•	Responsible for Ingesting data from diverse sources (like operational DB, Third Party API, Streaming Data, and Partner Data) and storing it as S3 objects in the data lake and then using AWS Glue to process ingested datasets until they’re in a consumable state. Used ETL to develop jobs for extracting, cleaning, transforming & loading data into data warehouse.  
•	Developed personalization platform serving millions of customers that consumes tens of billions of user-based events/logs and builds the datasets required for personalization. Designed and implemented the key components of the platform to perform real time ingestion of the events from over 75 data sources both in structured and unstructured form. Worked on complex SQL Queries, PL/SQL procedures and convert them to ETL tasks. 
•	Responsible for developing bigdata web application using Agile, built scalable distributed data solutions using Hadoop, worked with different data sources like Teradata for Spark to process data and used database to store application data.
•	Use Kafka as publish-subscribe messaging system, developed end to end data processing pipelines begin with receiving data, and created topics using consumers and producers to ingest data into application for PySpark to process data.
•	Utilized SQL*Loader to perform bulk data loads into database tables from external data files. 
•	Designed and Developed Real time Stream processing Application using Spark, and Hive to perform Streaming ETL. 
•	Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting on the dashboard. Also, writing Hive join query to fetch info from multiple tables, writing multiple Map Reduce jobs to collect output from Hive.
•	Designed AWS Lambda functions in Python for triggering the shell script to ingest the data into Mongo DB and exporting data from Mongo DB to consumers and created an AWS Lambda architecture to monitor AWS S3 Buckets.
•	Generated ETL code in Python to extract data from source using AWS Glue, transformed it to match target schema, and loaded into target using Amazon Redshift. 
•	Worked with Docker as well as Kubernetes on the multiple cloud providers, from helping developers build and containerize their application (CI/ CD) pipelines to deploying either on public or private cloud. 
•	Created an on premise CI/CD solution using Jenkins and the Pipeline plugin which uses pipeline as code. Involved in a Docker deployment pipeline for custom application images in the private cloud using Jenkins.
•	Used Pandas API to put the data as time series and tabular format for east timestamp data manipulation and retrieval. 
•	Developed a python application for smooth product order and delivery with python on backend side & Angular.js on front-end side along with Django to secure functionality & provide authentication for Rest API, ran it on AWS cloud.

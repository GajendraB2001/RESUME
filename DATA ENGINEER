•	Responsible for Ingesting data from diverse sources (like operational DB, Third Party API, Streaming Data, and Partner Data) and storing it as S3 objects in the data lake and then using AWS Glue to process ingested datasets until they’re in a consumable state. Used ETL to develop jobs for extracting, cleaning, transforming & loading data into data warehouse.  
•	Developed personalization platform serving millions of customers that consumes tens of billions of user-based events/logs and builds the datasets required for personalization. Designed and implemented the key components of the platform to perform real time ingestion of the events from over 75 data sources both in structured and unstructured form. Worked on complex SQL Queries, PL/SQL procedures and convert them to ETL tasks. 
•	Responsible for developing bigdata web application using Agile, built scalable distributed data solutions using Hadoop, worked with different data sources like Teradata for Spark to process data and used database to store application data.
•	Use Kafka as publish-subscribe messaging system, developed end to end data processing pipelines begin with receiving data, and created topics using consumers and producers to ingest data into application for PySpark to process data.
•	Utilized SQL*Loader to perform bulk data loads into database tables from external data files. 
•	Designed and Developed Real time Stream processing Application using Spark, and Hive to perform Streaming ETL. 
•	Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting on the dashboard. Also, writing Hive join query to fetch info from multiple tables, writing multiple Map Reduce jobs to collect output from Hive.
•	Designed AWS Lambda functions in Python for triggering the shell script to ingest the data into Mongo DB and exporting data from Mongo DB to consumers and created an AWS Lambda architecture to monitor AWS S3 Buckets.
•	Generated ETL code in Python to extract data from source using AWS Glue, transformed it to match target schema, and loaded into target using Amazon Redshift. 
•	Worked with Docker as well as Kubernetes on the multiple cloud providers, from helping developers build and containerize their application (CI/ CD) pipelines to deploying either on public or private cloud. 
•	Created an on premise CI/CD solution using Jenkins and the Pipeline plugin which uses pipeline as code. Involved in a Docker deployment pipeline for custom application images in the private cloud using Jenkins.
•	Used Pandas API to put the data as time series and tabular format for east timestamp data manipulation and retrieval. 
•	Developed a python application for smooth product order and delivery with python on backend side & Angular.js on front-end side along with Django to secure functionality & provide authentication for Rest API, ran it on AWS cloud.
•	Implemented oracle EDM for migrating data of both the SCADA system and cloud into a single system after defining multiple business rules for different kinds of data and ensured that the source data matched with the destination data.
•	Scheduled data ingestion, transformations, and reporting processes by Automating data workflows and pipeline using Apache Airflow. 
•	Designed and implemented Python scripts to extract data from Oracle EDM repositories, transforming it into usable formats for downstream applications. 
•	Developed RESTful APIs using Python, allowing Oracle EDM to communicate with other applications and systems for real-time data updates and retrieval.
•	Implemented robust error-handling mechanisms and conducted troubleshooting to identify and resolve issues related to Python-Oracle EDM integration.
•	Integrated Oracle EDM data with Azure Data Lake Storage for efficient data storage, retrieval, and analytics capabilities.
•	Utilized Azure Data Factory to automate data extraction, transformation, and loading (ETL) processes, enhancing data management within Oracle EDM.
•	Orchestrated data pipelines using Azure Data Factory ensuring smooth data flow between Oracle EDM and Azure data repositories.
•	Generated ETL code in Python to extract data from source using Azure Data Factory, transformed it to match target schema, and loaded into target using Azure Synapse Analytics. Developed ETL pipeline including data auditing, data cleansing, data de-duplication, and data transformations for business analytics using PySpark and Azure blob. 
•	Wrote end to end data migration scripts and performing data transformation/cleansing using ETL tool. Created and automated ETL processes to pull millions of records from various sources and supporting Decision Analytics to derive consumer behavior.
•	Performed Data Cleaning, features scaling, features engineering using pandas and NumPy packages in python.
•	Developing and implementing Spark ETL jobs for various data sources in distributed environment & exposing data to product users via BI visualization tools. Technologies leveraged included PySpark & MySQL
•	Developed a web-based application using Flask to streamline the fraud detection process for auto insurance claims submitted.
•	Applied an ensemble of classification models to identify potential fraudulent claims and run models on data queried from data warehouse and subjected them to further investigation by employees. Worked with Billions of records as part of Analytics and Modeling on migration of Data Warehouses from on-prem to cloud to create a data lake.
•	Automated and streamlined multi-asset enterprise data-transformation-testing through development of Python-powered utilities. Built pipelines for processing data, implementing cloud architecture both for real time and batch processing of application data from multiple data marts in HDFS for analytics.
•	Involved in development of Web Services using REST API’s for sending and getting data from external interface in JSON format. Conducted POC's on migrating to PySpark & Spark-Streaming using KAFKA to process live data streams. 
•	Used Informatica to create source/target definitions and sessions to ETL data into staging tables from sources. 
•	Migrated existing on-premises workloads to AWS. Ingested data from various source(Operational DB, 3rd Party API), stored it as S3 objects in data lake & then used AWS to process ingested datasets until they’re in consumed state.
•	Implemented a continuous Delivery(CD) pipeline with Jenkins and GitHub, whenever a new GitHub branches are created Jenkins automatically attempts to build a new Docker container from it. 

